{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import seaborn\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull data from Mongodb server and return Numpy arrays\n",
    "def gather_mongo_data():\n",
    "    client = MongoClient('127.0.0.1', 3001)\n",
    "    db = client.meteor\n",
    "    images = list(db.facebook.find({}))\n",
    "\n",
    "    df = pd.DataFrame(images)\n",
    "    df = df.dropna(subset = ['normalized_log_likes'])\n",
    "    # drop data with missing username\n",
    "    df = df[df['user'] != 'profile.php']\n",
    "\n",
    "    # build numpy array from DataFrame\n",
    "    # there has to be a better way to do this -- I haven't investigated it yet\n",
    "    likes = np.zeros((len(df), 1))\n",
    "    pool = np.zeros((len(df), 2048))\n",
    "    categories = np.zeros((len(df), 1008))\n",
    "    facedata = np.zeros((len(df), 3))\n",
    "\n",
    "    j = 0\n",
    "    for i in df.index:\n",
    "        likes[j, :] = df['normalized_log_likes'][i]\n",
    "        pool[j, :] = df['inception_pool'][i]\n",
    "        categories[j, :] = df['inception_classification'][i]\n",
    "        facedata[j, :] = [df['faces'][i]['num'], df['faces'][i]['total'], df['faces'][i]['largest']]\n",
    "        j += 1\n",
    "\n",
    "    users = set(df['user'])\n",
    "    users = list(users)\n",
    "    user_hot = np.zeros((len(df), len(users)))\n",
    "    user_num = np.zeros(len(df))\n",
    "\n",
    "    j = 0\n",
    "    for i in df.index:\n",
    "        user_index = users.index(df['user'][i])\n",
    "        user_num[j] = user_index\n",
    "        user_hot[j, user_index] = 1\n",
    "        j += 1\n",
    "\n",
    "    predictors = np.hstack((pool, facedata))\n",
    "    \n",
    "    return (predictors, likes, users, user_hot, user_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the n*(n-1)/2 comparisons of elements and measure how many of them are made correctly\n",
    "def correct_comparisons(y, pred_y):\n",
    "    comparison_true = (y.reshape(1,-1) - y.reshape(-1, 1)) > 0\n",
    "    comparison_est = (pred_y.reshape(1,-1) - pred_y.reshape(-1,1)) > 0\n",
    "        \n",
    "    return ((np.sum(comparison_true == comparison_est) - len(y))/2, (len(y)**2 - len(y))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate all correct comparisons across test user set\n",
    "def calculate_correct_comparisons(test_observations, test_users, user_hot, predicted_likes, likes):\n",
    "    total_correct = 0\n",
    "    total_comparisons = 0\n",
    "\n",
    "    test_user_hot = user_hot[test_observations, :]\n",
    "    \n",
    "    for i in range(len(test_users)):\n",
    "        user_test_set = np.any(test_user_hot[:, [test_users[i]]], axis = 1).nonzero()[0]\n",
    "        \n",
    "        ypred = np.ravel(predicted_likes[user_test_set])\n",
    "        y = np.ravel(likes[user_test_set])\n",
    "\n",
    "        (correct, total) = correct_comparisons(y, ypred)\n",
    "        total_correct += correct\n",
    "        total_comparisons += total\n",
    "\n",
    "    return float(total_correct)/total_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the facebook dataset into training, validation, and test components\n",
    "\n",
    "def split_datasets(users, user_hot, predictors, likes, seed=5):\n",
    "    print('Seed value is: ' + str(seed))\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # choose 100 random users to be the test set\n",
    "    test_users = np.random.choice(len(users), 100)\n",
    "    test_set = np.any(user_hot[:, test_users], axis = 1).nonzero()[0]\n",
    "    # choose ~100 random users to be the validation set\n",
    "    validation_users = [v for v in np.random.choice(len(users), 103) if v not in test_users]\n",
    "    validation_set = np.any(user_hot[:, validation_users], axis = 1).nonzero()[0]\n",
    "    \n",
    "    # training set is everything left\n",
    "    training_users = [v for v in range(len(users)) if v not in test_users and v not in validation_users]\n",
    "    training_set = [v for v in range(len(predictors)) if v not in test_set and v not in validation_set]\n",
    "\n",
    "    print(\"Training set length: \" + str(len(training_set)))\n",
    "    print(\"Test set length: \" + str(len(test_set)))\n",
    "    print(\"Validation set length: \" + str(len(validation_set)))\n",
    "    \n",
    "    return {\"training\": {\"observations\": training_set, \"users\": training_users, \"X\": predictors[training_set, :], \"y\": likes[training_set]}, \n",
    "            \"validation\": {\"observations\": validation_set, \"users\": validation_users, \"X\": predictors[validation_set, :], \"y\": likes[validation_set]}, \n",
    "            \"test\": {\"observations\": test_set, \"users\": test_users, \"X\": predictors[test_set, :], \"y\": likes[test_set]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(predictors, likes, users, user_hot, user_num) = gather_mongo_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value is: 5\n",
      "Training set length: 68280\n",
      "Test set length: 1851\n",
      "Validation set length: 1435\n"
     ]
    }
   ],
   "source": [
    "data = split_datasets(users, user_hot, predictors, likes, seed=5)\n",
    "\n",
    "training_data = xgb.DMatrix(data[\"training\"][\"X\"], label=data[\"training\"][\"y\"])\n",
    "validation_data = xgb.DMatrix(data[\"validation\"][\"X\"], label=data[\"validation\"][\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 0.3,1, 0.5\n",
      "   ! new best score: 0.440609\n",
      "params: 0.3,2, 0.5\n",
      "params: 0.3,3, 0.5\n",
      "params: 0.3,4, 0.5\n",
      "params: 0.3,6, 0.5\n",
      "params: 0.1,1, 0.5\n",
      "params: 0.1,2, 0.5\n",
      "   ! new best score: 0.436762\n",
      "params: 0.1,3, 0.5\n",
      "params: 0.1,4, 0.5\n",
      "params: 0.1,6, 0.5\n",
      "params: 0.03,1, 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-55e51359f894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                       \u001b[0mfeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcalculate_correct_comparisons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"users\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                       early_stopping_rounds=num_early_stop, verbose_eval=False)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    201\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = 1\n",
    "best_params = []\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "for subsample in [0.5, 1]:\n",
    "    for max_depth in [1, 2, 3, 4, 6]:\n",
    "        params = {\"nthread\": 4, \"eta\": eta, \"max_depth\": max_depth, \"subsample\": subsample, \"silent\": 1}\n",
    "        print(\"params: \" + str(eta) + \",\" + str(max_depth) + \", \" + str(subsample))\n",
    "\n",
    "        num_early_stop = 50\n",
    "\n",
    "        trained = xgb.train(params, training_data, num_boost_round=5000, evals = [[validation_data, \"validation\"]], \n",
    "                  feval = lambda preds, dtrain: \n",
    "                            list([[\"error\", (1 - calculate_correct_comparisons(data[\"validation\"][\"observations\"], data[\"validation\"][\"users\"], user_hot, preds, dtrain.get_label()))]]),\n",
    "                  early_stopping_rounds=num_early_stop, verbose_eval=False)\n",
    "\n",
    "        if trained.best_score < best_score:\n",
    "            print(\"   ! new best score: \" + str(trained.best_score))\n",
    "            best_score = trained.best_score\n",
    "            best_params = [max_depth, eta, subsample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value is: 5\n",
      "Training set length: 68280\n",
      "Test set length: 1851\n",
      "Validation set length: 1435\n",
      "params: 0.1,1, 0.5\n",
      "   ! new best score: 0.444363\n",
      "params: 0.1,2, 0.5\n",
      "   ! new best score: 0.437438\n",
      "params: 0.1,3, 0.5\n",
      "   ! new best score: 0.428826\n",
      "params: 0.1,4, 0.5\n",
      "   ! new best score: 0.426167\n",
      "params: 0.1,6, 0.5\n",
      "params: 0.1,1, 1\n",
      "params: 0.1,2, 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-61ca1851da1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                   \u001b[0mfeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcalculate_correct_comparisons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"users\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                   early_stopping_rounds=num_early_stop, verbose_eval=False)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    201\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = 1\n",
    "best_params = []\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "norm_likes = likes\n",
    "\n",
    "for j in range(user_hot.shape[1]):\n",
    "        like_error = np.mean(likes[user_hot[:,j] == 1])\n",
    "        norm_likes[user_hot[:,j] == 1] -= like_error\n",
    "\n",
    "norm_data = split_datasets(users, user_hot, predictors, norm_likes, seed=5)\n",
    "training_data = xgb.DMatrix(norm_data[\"training\"][\"X\"], label=norm_data[\"training\"][\"y\"])\n",
    "        \n",
    "for subsample in [0.5, 1]:\n",
    "    for max_depth in [1, 2, 3, 4, 6]:\n",
    "        params = {\"nthread\": 4, \"eta\": eta, \"max_depth\": max_depth, \"subsample\": subsample, \"silent\": 1}\n",
    "        print(\"params: \" + str(eta) + \",\" + str(max_depth) + \", \" + str(subsample))\n",
    "\n",
    "        num_early_stop = 50\n",
    "\n",
    "        trained = xgb.train(params, training_data, num_boost_round=5000, evals = [[validation_data, \"validation\"]], \n",
    "                  feval = lambda preds, dtrain: \n",
    "                            list([[\"error\", (1 - calculate_correct_comparisons(data[\"validation\"][\"observations\"], data[\"validation\"][\"users\"], user_hot, preds, dtrain.get_label()))]]),\n",
    "                  early_stopping_rounds=num_early_stop, verbose_eval=False)\n",
    "\n",
    "        if trained.best_score < best_score:\n",
    "            print(\"   ! new best score: \" + str(trained.best_score))\n",
    "            best_score = trained.best_score\n",
    "            best_params = [max_depth, eta, subsample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-error:0.477304\n",
      "Will train until validation-error hasn't improved in 40 rounds.\n"
     ]
    }
   ],
   "source": [
    "trained = xgb.train(params, training_data, num_boost_round=1, evals = [[validation_data, \"validation\"]], \n",
    "                      feval = lambda preds, dtrain: \n",
    "                                list([[\"error\", (1 - calculate_correct_comparisons(data[\"validation\"][\"observations\"], data[\"validation\"][\"users\"], user_hot, preds, dtrain.get_label()))]]),\n",
    "                      early_stopping_rounds=40, verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value is: 5\n",
      "Training set length: 68280\n",
      "Test set length: 1851\n",
      "Validation set length: 1435\n",
      "starting score: 0.499720184057\n",
      "params: 0.05,4, 1.0\n",
      "   ! new score: 0.55227117274\n",
      "params: 0.0495,4, 1.0\n",
      "   ! new score: 0.553351573187\n",
      "params: 0.049005,4, 1.0\n",
      "   ! new score: 0.554773970899\n",
      "params: 0.04851495,4, 1.0\n",
      "   ! new score: 0.555007150852\n",
      "params: 0.0480298005,4, 1.0\n",
      "   ! new score: 0.557136861087\n",
      "params: 0.047549502495,4, 1.0\n",
      "   ! new score: 0.55778199229\n",
      "params: 0.04707400747,4, 1.0\n",
      "   ! new score: 0.557929672926\n",
      "params: 0.0466032673953,4, 1.0\n",
      "   ! new score: 0.558598122124\n",
      "params: 0.0461372347214,4, 1.0\n",
      "   ! new score: 0.559196617336\n",
      "params: 0.0456758623742,4, 1.0\n",
      "   ! new score: 0.560727832359\n",
      "params: 0.0452191037504,4, 1.0\n",
      "   ! new score: 0.560510197737\n",
      "params: 0.0447669127129,4, 1.0\n",
      "   ! new score: 0.560867740331\n",
      "params: 0.0443192435858,4, 1.0\n",
      "   ! new score: 0.561023193633\n",
      "params: 0.0438760511499,4, 1.0\n",
      "   ! new score: 0.561372963562\n",
      "params: 0.0434372906384,4, 1.0\n",
      "   ! new score: 0.561225282925\n",
      "params: 0.0430029177321,4, 1.0\n",
      "   ! new score: 0.561497326203\n",
      "params: 0.0425728885547,4, 1.0\n",
      "   ! new score: 0.561808232807\n",
      "params: 0.0421471596692,4, 1.0\n",
      "   ! new score: 0.56204141276\n",
      "params: 0.0417256880725,4, 1.0\n",
      "   ! new score: 0.562787588608\n",
      "params: 0.0413084311918,4, 1.0\n",
      "   ! new score: 0.56248445467\n",
      "params: 0.0408953468799,4, 1.0\n",
      "   ! new score: 0.561784914812\n",
      "params: 0.0404863934111,4, 1.0\n",
      "   ! new score: 0.563114040542\n",
      "params: 0.040081529477,4, 1.0\n",
      "   ! new score: 0.563479355802\n",
      "params: 0.0396807141822,4, 1.0\n",
      "   ! new score: 0.563098495212\n",
      "params: 0.0392839070404,4, 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-47df37aa8674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         trained = xgb.train(params, training_data, num_boost_round=10, \n\u001b[0;32m---> 23\u001b[0;31m               verbose_eval=True, xgb_model='save.xgb')\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         trained = xgb.train(params, training_data, num_boost_round=10,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    201\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "norm_data = split_datasets(users, user_hot, predictors, likes, seed=5)\n",
    "\n",
    "training_data = xgb.DMatrix(norm_data[\"training\"][\"X\"], label=norm_data[\"training\"][\"y\"])\n",
    "validation_data = xgb.DMatrix(norm_data[\"validation\"][\"X\"], label=norm_data[\"validation\"][\"y\"])\n",
    "\n",
    "eta = 0.05\n",
    "subsample = 1.0\n",
    "max_depth = 4\n",
    "\n",
    "score = calculate_correct_comparisons(norm_data[\"validation\"][\"observations\"], norm_data[\"validation\"][\"users\"], user_hot, norm_data[\"validation\"][\"y\"]*0, norm_data[\"validation\"][\"y\"])\n",
    "print(\"starting score: \" + str(score))\n",
    "\n",
    "user_train = user_hot[norm_data[\"training\"][\"observations\"], :]\n",
    "norm_likes = likes[norm_data[\"training\"][\"observations\"]]\n",
    "\n",
    "for i in range(50):\n",
    "    params = {\"nthread\": 4, \"eta\": eta, \"max_depth\": max_depth, \"subsample\": subsample, \"silent\": 1}\n",
    "    print(\"params: \" + str(eta) + \",\" + str(max_depth) + \", \" + str(subsample))\n",
    "\n",
    "\n",
    "    if (i != 0):\n",
    "        trained = xgb.train(params, training_data, num_boost_round=10, \n",
    "              verbose_eval=True, xgb_model='save.xgb')\n",
    "    else:\n",
    "        trained = xgb.train(params, training_data, num_boost_round=10,\n",
    "                  verbose_eval=True)\n",
    "\n",
    "    trained.save_model('save.xgb')\n",
    "    \n",
    "    score = calculate_correct_comparisons(norm_data[\"validation\"][\"observations\"], norm_data[\"validation\"][\"users\"], user_hot, trained.predict(validation_data), norm_data[\"validation\"][\"y\"])\n",
    "    \n",
    "    pred_likes = trained.predict(training_data)\n",
    "    like_errors = []\n",
    "    \n",
    "    for j in norm_data[\"training\"][\"users\"]:\n",
    "        like_error = np.mean(norm_likes[user_train[:, j] == 1] - pred_likes[user_train[:,j] == 1])\n",
    "        like_errors.append(like_error)\n",
    "        norm_likes[user_train[:,j] == 1] -= like_error * 0.2\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.hist(like_errors)\n",
    "#     plt.show()\n",
    "        \n",
    "    training_data = xgb.DMatrix(norm_data[\"training\"][\"X\"], label=norm_likes)\n",
    "    \n",
    "    eta = eta * 1.07\n",
    "    \n",
    "    print(\"   ! new score: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value is: 5\n",
      "Training set length: 68280\n",
      "Test set length: 1851\n",
      "Validation set length: 1435\n",
      "starting score: 0.499720184057\n",
      "params: 0.05,4, 1.0\n",
      "   10-score: 0.55227117274\n",
      "params: 0.0535,4, 1.0\n",
      "   20-score: 0.554369792314\n",
      "params: 0.057245,4, 1.0\n",
      "   30-score: 0.55485169755\n",
      "params: 0.06125215,4, 1.0\n",
      "   40-score: 0.555877689342\n",
      "params: 0.0655398005,4, 1.0\n",
      "   50-score: 0.55570669071\n",
      "params: 0.070127586535,4, 1.0\n",
      "   60-score: 0.556895908469\n",
      "params: 0.0750365175925,4, 1.0\n",
      "   70-score: 0.557408904365\n",
      "params: 0.0802890738239,4, 1.0\n",
      "   80-score: 0.558706939435\n",
      "params: 0.0859093089916,4, 1.0\n",
      "   90-score: 0.56120196493\n",
      "params: 0.091922960621,4, 1.0\n",
      "   100-score: 0.56164500684\n",
      "params: 0.0983575678645,4, 1.0\n",
      "   110-score: 0.561124238279\n",
      "params: 0.105242597615,4, 1.0\n",
      "   120-score: 0.561761596816\n",
      "params: 0.112609579448,4, 1.0\n",
      "   130-score: 0.563215085188\n",
      "params: 0.120492250009,4, 1.0\n",
      "   140-score: 0.563728081084\n",
      "params: 0.12892670751,4, 1.0\n",
      "   150-score: 0.565585748041\n",
      "params: 0.137951577036,4, 1.0\n",
      "   160-score: 0.56422553165\n",
      "params: 0.147608187428,4, 1.0\n",
      "   170-score: 0.564777390872\n",
      "params: 0.157940760548,4, 1.0\n",
      "   180-score: 0.565344795423\n",
      "params: 0.168996613787,4, 1.0\n",
      "   190-score: 0.56462193757\n",
      "params: 0.180826376752,4, 1.0\n",
      "   200-score: 0.564691891556\n",
      "params: 0.193484223124,4, 1.0\n",
      "   210-score: 0.565111615471\n",
      "params: 0.207028118743,4, 1.0\n",
      "   220-score: 0.563658127099\n",
      "params: 0.221520087055,4, 1.0\n",
      "   230-score: 0.561404054222\n",
      "params: 0.237026493149,4, 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-8f2668404db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         trained = xgb.train(params, training_data, num_boost_round=per_round, \n\u001b[0;32m---> 24\u001b[0;31m               verbose_eval=True, xgb_model='save.xgb')\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         trained = xgb.train(params, training_data, num_boost_round=per_round,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    201\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "norm_data = split_datasets(users, user_hot, predictors, likes, seed=5)\n",
    "\n",
    "training_data = xgb.DMatrix(norm_data[\"training\"][\"X\"], label=norm_data[\"training\"][\"y\"])\n",
    "validation_data = xgb.DMatrix(norm_data[\"validation\"][\"X\"], label=norm_data[\"validation\"][\"y\"])\n",
    "\n",
    "eta = 0.05\n",
    "subsample = 1.0\n",
    "max_depth = 4\n",
    "per_round = 10\n",
    "\n",
    "score = calculate_correct_comparisons(norm_data[\"validation\"][\"observations\"], norm_data[\"validation\"][\"users\"], user_hot, norm_data[\"validation\"][\"y\"]*0, norm_data[\"validation\"][\"y\"])\n",
    "print(\"starting score: \" + str(score))\n",
    "\n",
    "user_train = user_hot[norm_data[\"training\"][\"observations\"], :]\n",
    "norm_likes = likes[norm_data[\"training\"][\"observations\"]]\n",
    "\n",
    "for i in range(50):\n",
    "    params = {\"nthread\": 4, \"eta\": eta, \"max_depth\": max_depth, \"subsample\": subsample, \"silent\": 1}\n",
    "    print(\"params: \" + str(eta) + \",\" + str(max_depth) + \", \" + str(subsample))\n",
    "\n",
    "\n",
    "    if (i != 0):\n",
    "        trained = xgb.train(params, training_data, num_boost_round=per_round, \n",
    "              verbose_eval=True, xgb_model='save.xgb')\n",
    "    else:\n",
    "        trained = xgb.train(params, training_data, num_boost_round=per_round,\n",
    "                  verbose_eval=True)\n",
    "\n",
    "    trained.save_model('save.xgb')\n",
    "    \n",
    "    score = calculate_correct_comparisons(norm_data[\"validation\"][\"observations\"], norm_data[\"validation\"][\"users\"], user_hot, trained.predict(validation_data), norm_data[\"validation\"][\"y\"])\n",
    "    \n",
    "    pred_likes = trained.predict(training_data)\n",
    "    like_errors = []\n",
    "    \n",
    "    for j in norm_data[\"training\"][\"users\"]:\n",
    "        like_error = np.mean(norm_likes[user_train[:, j] == 1] - pred_likes[user_train[:,j] == 1])\n",
    "        like_errors.append(like_error)\n",
    "        norm_likes[user_train[:,j] == 1] -= like_error * 0.2\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.hist(like_errors)\n",
    "#     plt.show()\n",
    "        \n",
    "    training_data = xgb.DMatrix(norm_data[\"training\"][\"X\"], label=norm_likes)\n",
    "    \n",
    "    eta = eta * 1.07\n",
    "    \n",
    "    print(\"   \" + str((i+1)*per_round) + \"-score: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "       151           0.2185            3.87m\n",
      "       152           0.2184            3.58m\n",
      "       153           0.2183            3.56m\n",
      "       154           0.2182            3.16m\n",
      "       155           0.2182            2.77m\n",
      "       156           0.2181            2.13m\n",
      "       157           0.2180            1.55m\n",
      "       158           0.2179            1.01m\n",
      "       159           0.2178           29.57s\n",
      "       160           0.2178            0.00s\n",
      "0.565026099925\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       161           0.2177            3.62m\n",
      "       162           0.2176            3.29m\n",
      "       163           0.2176            2.84m\n",
      "       164           0.2175            2.44m\n",
      "       165           0.2174            2.05m\n",
      "       166           0.2173            1.65m\n",
      "       167           0.2172            1.23m\n",
      "       168           0.2172           49.02s\n",
      "       169           0.2171           24.47s\n",
      "       170           0.2170            0.00s\n",
      "0.563609246831\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       171           0.2169            3.77m\n",
      "       172           0.2168            3.32m\n",
      "       173           0.2168            2.89m\n",
      "       174           0.2167            2.47m\n",
      "       175           0.2166            2.05m\n",
      "       176           0.2165            1.64m\n",
      "       177           0.2164            1.23m\n",
      "       178           0.2163           49.05s\n",
      "       179           0.2162           24.52s\n",
      "       180           0.2162            0.00s\n",
      "0.56129753915\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       181           0.2161            3.64m\n",
      "       182           0.2160            3.23m\n",
      "       183           0.2159            2.85m\n",
      "       184           0.2159            2.43m\n",
      "       185           0.2158            2.03m\n",
      "       186           0.2157            1.62m\n",
      "       187           0.2157            1.21m\n",
      "       188           0.2156           48.70s\n",
      "       189           0.2155           24.42s\n",
      "       190           0.2155            0.00s\n",
      "0.559209545116\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       191           0.2154            3.64m\n",
      "       192           0.2153            3.26m\n",
      "       193           0.2152            2.85m\n",
      "       194           0.2151            2.46m\n",
      "       195           0.2151            2.04m\n",
      "       196           0.2150            1.64m\n",
      "       197           0.2149            1.23m\n",
      "       198           0.2148           49.27s\n",
      "       199           0.2148           24.62s\n",
      "       200           0.2147            0.00s\n",
      "0.560700969426\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       201           0.2146            4.43m\n",
      "       202           0.2145            4.50m\n",
      "       203           0.2145            3.78m\n",
      "       204           0.2144            3.11m\n",
      "       205           0.2143            2.53m\n",
      "       206           0.2143            2.10m\n",
      "       207           0.2142            1.59m\n",
      "       208           0.2141            1.07m\n",
      "       209           0.2141           31.90s\n",
      "       210           0.2140            0.00s\n",
      "0.562714392245\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       211           0.2139            4.78m\n",
      "       212           0.2138            4.48m\n",
      "       213           0.2138            4.02m\n",
      "       214           0.2137            3.54m\n",
      "       215           0.2136            2.97m\n",
      "       216           0.2135            3.99m\n",
      "       217           0.2135            2.79m\n",
      "       218           0.2134            1.83m\n",
      "       219           0.2133           52.94s\n",
      "       220           0.2132            0.00s\n",
      "0.56428038777\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       221           0.2131            5.11m\n",
      "       222           0.2130            4.57m\n",
      "       223           0.2130            4.19m\n",
      "       224           0.2129            3.53m\n",
      "       225           0.2128            2.92m\n",
      "       226           0.2127            2.37m\n",
      "       227           0.2127            1.80m\n",
      "       228           0.2126            1.22m\n",
      "       229           0.2126           36.31s\n",
      "       230           0.2125            0.00s\n",
      "0.563609246831\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       231           0.2124            6.22m\n",
      "       232           0.2123            4.91m\n",
      "       233           0.2122            4.58m\n",
      "       234           0.2122            3.90m\n",
      "       235           0.2121            3.16m\n",
      "       236           0.2121            2.48m\n",
      "       237           0.2120            1.85m\n",
      "       238           0.2120            1.24m\n",
      "       239           0.2119           35.97s\n",
      "       240           0.2118            0.00s\n",
      "0.563087248322\n",
      "      Iter       Train Loss   Remaining Time \n",
      "       241           0.2117            3.86m\n",
      "       242           0.2117            3.43m\n",
      "       243           0.2116            3.06m\n",
      "       244           0.2115            2.64m\n",
      "       245           0.2114            2.22m\n",
      "       246           0.2114            1.76m\n",
      "       247           0.2113            1.31m\n",
      "       248           0.2112           52.67s\n",
      "       249           0.2111           26.33s\n",
      "       250           0.2111            0.00s\n",
      "0.562341536167\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# gbr = GradientBoostingRegressor(learning_rate=0.1, n_estimators=1, max_depth=3, warm_start=True, verbose=1)\n",
    "\n",
    "for i in range(15, 25):\n",
    "    gbr.set_params(n_estimators = (i+1)*10)\n",
    "    gbr.fit(train_data, np.ravel(train_label))\n",
    "    \n",
    "    print(calculate_correct_comparisons(validation_users, user_hot, predictors, likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: non integer (and non boolean) array-likes will not be accepted as indices in the future\n"
     ]
    }
   ],
   "source": [
    "calculate_correct_comparisons(user_train, user_hot, predictors, likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71566, 5863)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
